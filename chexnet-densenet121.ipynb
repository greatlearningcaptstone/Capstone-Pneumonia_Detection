{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import all required libraries\nimport pandas as pd\nimport numpy as np\nimport pydicom\nimport pylab\nfrom skimage.transform import resize\nimport pathlib\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the path for training images\nTRAIN_IMAGES ='../input/rsna-pneumonia-detection-challenge/stage_2_train_images'\nDataset = '../input/rsna-pneumonia-detection-challenge'\n#weights = 'E:/Machine Learning/Great Learning/Projects/GL Capstone Project/GL Capstone Project/Code Base/ChexNet/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the training CSV File and remove duplicates on Patient Id\nfilepath = (Dataset+'/stage_2_train_labels.csv')\nImages_df = pd.read_csv(filepath)\nImages_model_df = Images_df[['patientId','Target']]\nImages_model_df=Images_model_df.drop_duplicates(subset='patientId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample the training images for initial experimentation\nImages_sample_df = Images_model_df.sample(frac=1.0,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the count\nImages_sample_df['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Don't Use this function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_dict(Images_sample_df,test_size,random_state=42): \n   # Split into train and test validation datasets\n    train_df, test_df = train_test_split(Images_sample_df, test_size=0.02, random_state=42, stratify=Images_sample_df[['Target']])\n   # Convert to dictionary with patient-id as key and target as value\n    train_dict=train_df.set_index('patientId')['Target'].to_dict()\n    test_dict=test_df.set_index('patientId')['Target'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test validation datasets\ntrain_df, test_df = train_test_split(Images_sample_df, test_size=0.02, random_state=42, stratify=Images_sample_df[['Target']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to dictionary with patient-id as key and target as value\ntrain_dict=train_df.set_index('patientId')['Target'].to_dict()\ntest_dict=test_df.set_index('patientId')['Target'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Custom Generator Class to be used in Model Generator\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, path,batch_size=128, dim=(224,224), n_channels=3,\n                 n_classes=1, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.path = path\n        self.on_epoch_end()\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            dcm_file_sample = (self.path +\"/\"+ ID +\".dcm\")\n            dcm_data_sample = pydicom.filereader.dcmread(dcm_file_sample)\n            image = dcm_data_sample.pixel_array\n            image_array = np.stack([image] * 3, axis=2)\n            image_array = image_array / 255.\n            image_array = resize(image_array, (224, 224), mode= 'constant', anti_aliasing=True)\n            X[i,] = image_array\n\n            # Store class\n            y[i] = self.labels[ID]\n\n        return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the DenseNet model pre-loaded with imagenet weights with last layer set as false\ninput_shape = (224, 224, 3)\nnum_of_class=1\nimg_in = Input(input_shape)              \nmodel = DenseNet121(include_top= False, \n                weights='imagenet',    \n                input_tensor= img_in, \n                input_shape= input_shape,\n                pooling ='avg') \n\n# The pre-trained model has classification output for 14 categories and hence Dense layer is defined with layer 14\nx = model.output  \npredictions = Dense(14, activation=\"sigmoid\", name=\"predictions\")(x)    \nmodel = Model(inputs=img_in, outputs=predictions)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the last dense layer of 14 classes and print the summary\nmodel.layers.pop()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a new dense layer of 1 class and chain the previous layer output to new model\nnew_layer = Dense(1, activation=\"sigmoid\", name=\"my_predictions\")    \ninp = model.input\nout = new_layer(model.layers[-1].output)\nmodel2 = Model(inp, out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print new model summary\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Train and Test generator\ntrain_generator = DataGenerator(list(train_dict.keys()), train_dict,path=TRAIN_IMAGES,batch_size=32)\nvalidation_generator = DataGenerator(list(test_dict.keys()), test_dict,path=TRAIN_IMAGES,batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define Custom Metrics Functions to be used in Keras Training\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set Early stopping parameter and Reduce Learning rate on Plateau\ncallbacks_list = [EarlyStopping(monitor='val_loss',patience=5,),\n                  ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,),\n                  ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=2,)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set only the last layer as Trainable\ndef model_train_layers(model,layer):\n    model2.trainable = True\n    set_trainable = False\n    for layer in model2.layers:\n      #print(layer.name)\n        if layer.name == layer:\n            set_trainable = True\n        if set_trainable:\n             layer.trainable = True\n        else:\n             layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_train_layers(model2,\"my_predictions\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile with binary cross entropy loss\noptimizer = Adam(lr=0.001)\nmodel2.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc',f1_m,precision_m, recall_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run Fit Generator\nhistory=model2.fit_generator(generator=train_generator,\n                    epochs=2,\n                    validation_data=validation_generator,\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the results on Loss and Accuracy\nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though training accuracy and Validation accuracy are in the range of 80%, the precision and recall scores are very low\nfor training and validation cases. This could be more to do with the imbalance of positive and negative cases in the training dataset(20-80 ratio)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Pos_df = Images_sample_df[Images_sample_df['Target']==1]\nNeg_df = Images_sample_df[Images_sample_df['Target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Neg_sample_df = Neg_df.sample(frac=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Images_corr_Sample_df = pd.concat([Pos_df,Neg_sample_df],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test validation datasets\ntrain_data, test_data = train_test_split(Images_corr_Sample_df, test_size=0.02, random_state=42, stratify=Images_sample_df[['Target']])\n# Convert to dictionary with patient-id as key and target as value\ntrain_dr=train_data.set_index('patientId')['Target'].to_dict()\ntest_dr=test_data.set_index('patientId')['Target'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Train and Test generator\ntrain_generator = DataGenerator(list(train_dr.keys()), train_dr,path=TRAIN_IMAGES,batch_size=32)\nvalidation_generator = DataGenerator(list(test_dr.keys()), test_dr,path=TRAIN_IMAGES,batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_test_dict(Images_corr_Sample_df,test_size=0.02,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run Fit Generator\nhistory=model2.fit_generator(generator=train_generator,\n                    epochs=2,\n                    validation_data=validation_generator,\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_train_layers(model2,\"conv5_block16_0_bn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=0.001)\nmodel2.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc',f1_m,precision_m, recall_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model2.fit_generator(generator=train_generator,\n                    epochs=2,\n                    validation_data=validation_generator,\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}