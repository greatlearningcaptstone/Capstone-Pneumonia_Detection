{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import pylab\n",
    "from skimage.transform import resize\n",
    "import pathlib\n",
    "import keras\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pylab as plt\n",
    "import pydicom\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the current working directory\n",
    "import os; \n",
    "os.chdir('E:/rsna-pneumonia-detection-challenge/')\n",
    "#project_path =  \"Sarcasm Detection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Training images ['0004cfab-14fd-4e49-80ba-63a80b6bddd6.dcm', '000924cf-0f8d-42bd-9158-1af53881a557.dcm', '000db696-cf54-4385-b10b-6b16fbb3f985.dcm', '000fe35a-2649-43d4-b027-e67796d412e0.dcm', '001031d9-f904-4a23-b3e5-2c088acd19c6.dcm']\n"
     ]
    }
   ],
   "source": [
    "train_images_dir = 'E:/rsna-pneumonia-detection-challenge/stage_2_train_images/'\n",
    "train_images = [f for f in listdir(train_images_dir) if isfile(join(train_images_dir, f))]\n",
    "test_images_dir = 'E:/rsna-pneumonia-detection-challenge/stage_2_test_images/'\n",
    "test_images = [f for f in listdir(test_images_dir) if isfile(join(test_images_dir, f))]\n",
    "print('5 Training images', train_images[:5]) # Print the first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 26684\n",
      "Number of test images: 3000\n"
     ]
    }
   ],
   "source": [
    "print('Number of train images:', len(train_images))\n",
    "print('Number of test images:', len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for training images\n",
    "#TRAIN_IMAGES ='E:/rsna-pneumonia-detection-challenge/stage_2_train_images/'\n",
    "Dataset = 'E:/rsna-pneumonia-detection-challenge/'\n",
    "weights = 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training CSV File and remove duplicates on Patient Id\n",
    "filepath = (Dataset+'/stage_2_train_labels.csv')\n",
    "Images_df = pd.read_csv(filepath)\n",
    "Images_model_df = Images_df[['patientId','Target']]\n",
    "#Images_model_df=Images_model_df.drop_duplicates(subset='patientId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the training images for initial experimentation\n",
    "Images_sample_df = Images_model_df.sample(frac=1.0,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count\n",
    "Images_model_df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_sample_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test validation datasets\n",
    "train_df, test_df = train_test_split(Images_model_df, test_size=0.02, random_state=42, stratify=Images_model_df[['Target']])\n",
    "# Convert to dictionary with patient-id as key and target as value\n",
    "train_dict=train_df.set_index('patientId')['Target'].to_dict()\n",
    "test_dict=test_df.set_index('patientId')['Target'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Generator Class to be used in Model Generator\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, path,batch_size=128, dim=(224,224), n_channels=3,\n",
    "                 n_classes=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.path = path\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dcm_file_sample = (self.path +\"/\"+ ID +\".dcm\")\n",
    "            dcm_data_sample = pydicom.filereader.dcmread(dcm_file_sample)\n",
    "            image = dcm_data_sample.pixel_array\n",
    "            image_array = np.stack([image] * 3, axis=2)\n",
    "            image_array = image_array / 255.\n",
    "            image_array = resize(image_array, (224, 224), mode= 'constant', anti_aliasing=True)\n",
    "            X[i,] = image_array\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, labels, image_path, mask_path,\n",
    "                 to_fit=True, batch_size=32, dim=(224,224),\n",
    "                 n_channels=3, n_classes=1, shuffle=True):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_X(list_IDs_temp)\n",
    "\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _generate_X(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self._load_grayscale_image(self.image_path + self.labels[ID])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        y = np.empty((self.batch_size, *self.dim), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            y[i,] = self._load_grayscale_image(self.mask_path + self.labels[ID])\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _load_grayscale_image(self, image_path):\n",
    "        \"\"\"Load grayscale image\n",
    "        :param image_path: path to image to load\n",
    "        :return: loaded image\n",
    "        \"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img / 255\n",
    "        return img\n",
    "    \n",
    "    \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test generator\n",
    "train_generator = DataGenerator(list(train_dict.keys()), train_dict, image_path, mask_path)#, batch_size=32)\n",
    "validation_generator = DataGenerator(list(test_dict.keys()), test_dict, mask_path, mask_path)#, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test generator\n",
    "train_generator = DataGenerator(list(train_dict.keys()), train_dict, train_images, batch_size=32)\n",
    "validation_generator = DataGenerator(list(test_dict.keys()), test_dict, test_images, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DenseNet model pre-loaded with imagenet weights with last layer set as false\n",
    "input_shape = (224, 224, 3)\n",
    "num_of_class=1\n",
    "img_in = Input(input_shape)              \n",
    "model = DenseNet121(include_top=False, \n",
    "                weights='imagenet',    \n",
    "                input_tensor= img_in, \n",
    "                input_shape= input_shape,\n",
    "                pooling ='avg') \n",
    "\n",
    "# The pre-trained model has classification output for 14 categories and hence Dense layer is defined with layer 14\n",
    "x = model.output  \n",
    "predictions = Dense(14, activation=\"sigmoid\", name=\"predictions\")(x)    \n",
    "model = Model(inputs=img_in, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights on similar dataset\n",
    "model.load_weights(weights+\"E:/rsna-pneumonia-detection-challenge/brucechou1983_CheXNet_Keras_0.3.0_weights.h5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Custom Metrics Functions to be used in Keras Training\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Early stopping parameter and Reduce Learning rate on Plateau\n",
    "callbacks_list = [EarlyStopping(monitor='val_loss',patience=5,),\n",
    "                  ModelCheckpoint(filepath=weights+'my_model.h5',monitor='val_loss',save_best_only=True,),\n",
    "                  ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=2,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set only the last layer as Trainable\n",
    "def model_train_layers(model,layer):\n",
    "    model.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "      #print(layer.name)\n",
    "        if layer.name == layer:\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "             layer.trainable = True\n",
    "        else:\n",
    "             layer.trainable = False\n",
    "        \n",
    "        return model_train_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_layers(model,\"my_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with binary cross entropy loss\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "#y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "#y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(generator=train_generator,\n",
    "                    epochs=7,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "                  directory='E:/rsna-pneumonia-detection-challenge/' + r'/stage_2_train_images',\n",
    "                  target_size=(224, 224), # resize to this size\n",
    "                  color_mode=\"rgb\", # for coloured images\n",
    "                  batch_size=1, # number of images to extract from folder for every batch\n",
    "                  class_mode=\"binary\", # classes to predict\n",
    "                  seed=2020 # to make the result reproducible\n",
    "                  )\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "  # convert to unsigned integers for plotting\n",
    "  image = next(train_generator)[0].astype('uint8')\n",
    "\n",
    "  # changing size from (1, 200, 200, 3) to (200, 200, 3) for plotting the image\n",
    "  image = np.squeeze(image)\n",
    "\n",
    "  # plot raw pixel data\n",
    "  ax[i].imshow(image)\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(datagen.flow(train_images, batch_size=batch_size), \n",
    "                    epochs=epochs, # one forward/backward pass of training data\n",
    "                    steps_per_epoch=x_train.shape[0]//batch_size, # number of images comprising of one epoch\n",
    "                    validation_data=(x_test, y_test), # data for validation\n",
    "                    validation_steps=x_test.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10, # rotation\n",
    "        width_shift_range=0.2, # horizontal shift\n",
    "        height_shift_range=0.2, # vertical shift\n",
    "        zoom_range=0.2, # zoom\n",
    "        horizontal_flip=True, # horizontal flip\n",
    "        brightness_range=[0.2,1.2]) # brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
